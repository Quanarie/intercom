{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Intercom â€“ Milestone 1 Notebook\n",
    "Data reading, EDA, spectrogram creation, and a simple CNN that trains until interrupted."
   ],
   "id": "4138e4f6bf44f31"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.143023Z",
     "start_time": "2025-12-17T16:51:43.140510Z"
    }
   },
   "source": [
    "import os, random, time, json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa, librosa.display, soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD"
   ],
   "id": "bd4d25761897c3bc",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters"
   ],
   "id": "f7f5a8423ba172ff"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.148518Z",
     "start_time": "2025-12-17T16:51:43.146358Z"
    }
   },
   "source": [
    "class Args: pass\n",
    "args = Args()\n",
    "\n",
    "args.data = \"./dataset\"\n",
    "args.sr = 16000\n",
    "args.n_mels = 64\n",
    "args.fixed_length = 200\n",
    "\n",
    "args.batch_size = 32\n",
    "args.lr = 1e-3\n",
    "args.weight_decay = 1e-4\n",
    "\n",
    "args.test_size = 0.15\n",
    "args.val_size = 0.15\n",
    "\n",
    "args.seed = int(time.time()) % 100000"
   ],
   "id": "fae26aeeea1f00ac",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities and Audio Functions"
   ],
   "id": "b672b2387ad8869"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.156441Z",
     "start_time": "2025-12-17T16:51:43.151031Z"
    }
   },
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def ensure_dir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "ensure_dir(\"./models\")\n",
    "ensure_dir(\"./outputs\")"
   ],
   "id": "d54fef960c619626",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.349168Z",
     "start_time": "2025-12-17T16:51:43.160380Z"
    }
   },
   "source": [
    "def build_filelist(root):\n",
    "    pairs = []\n",
    "    label_map = {\"not_allowed\": 0, \"allowed\": 1}\n",
    "\n",
    "    for class_name, label in label_map.items():\n",
    "        class_dir = Path(root) / class_name\n",
    "        for f in class_dir.rglob(\"*.wav\"):\n",
    "            pairs.append((str(f), label))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "pairs = build_filelist(args.data)\n",
    "print(\"Total files:\", len(pairs))"
   ],
   "id": "8327b60b3d3db73c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 41490\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ],
   "id": "7b7b30bc2ed1fec5"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.375987Z",
     "start_time": "2025-12-17T16:51:43.354554Z"
    }
   },
   "source": [
    "allowed = [p for p in pairs if p[1] == 1]\n",
    "not_allowed = [p for p in pairs if p[1] == 0]\n",
    "\n",
    "min_len = min(len(allowed), len(not_allowed))\n",
    "allowed = random.sample(allowed, min_len)\n",
    "not_allowed = random.sample(not_allowed, min_len)\n",
    "\n",
    "pairs = allowed + not_allowed\n",
    "random.shuffle(pairs)\n",
    "\n",
    "print(\"Balanced dataset size:\", len(pairs))"
   ],
   "id": "9facaed09177c142",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset size: 41142\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.490062Z",
     "start_time": "2025-12-17T16:51:43.379872Z"
    }
   },
   "source": [
    "durations = [librosa.get_duration(path=p) for p, _ in pairs[:200]]\n",
    "plt.hist(durations, bins=40)\n",
    "plt.title(\"Audio Duration Distribution\")\n",
    "plt.show()"
   ],
   "id": "28e4ba496009ca47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGzCAYAAAAFROyYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAALWVJREFUeJzt3Qt4FOXZ//E7ISQcE0ggJJFAgMpJDFrQiCCCIAiUQ421AmqoFEEBC6kKsZaT9g0Fq7y1CK21oC2IYhEVlIocVQ4CigjFlCAIVg4eSgKhhJDM/7qf/7X77uacsJs82Xw/1zVudmZ2dubZkfntc5gNchzHEQAAAIsEV/cOAAAAFEZAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABfGDMmDGSkJDgNS8oKEhmzZpV68v36NGjpiyWLl1aI8qiT58+ZqoKhc8R/Vvnffvtt1Xy/nrO6rkL2IiAgoD33HPPmX/0k5KSpCbTY3BNISEhEhkZKd26dZNf/OIX8s9//rO6d0+WL18uCxYsEJvoxdez3Bo1aiRt27aVO+64Q/7+979LQUGBT95n27ZtJlycOXNGbGPzvgGlCSl1KRAAli1bZr4pfvTRR5KZmSk/+MEPquR9//vf/5og4Uu33nqr3HvvvaI/oZWVlSWffvqpvPjiiyaE/fa3v5XU1FSpzoCyf/9+mTJlitf81q1bm7KoW7dutexXWFiY/PnPfzZ/6358+eWX8tZbb5mQojUlb7zxhoSHh7vXf/fddysVAmbPnm0CUZMmTar1HKnIvmVkZEhwMN9TYScCCgLakSNHzD/Qq1atkvHjx5uwMnPmzCp573r16vl8m+3bt5e7777ba97cuXNl6NCh8stf/lI6duwogwcP9sl7nT9/Xho0aHDZ29GaC3+URXlpAChcZk8++aQpt7S0NBk3bpy88sor7mWhoaF+3R+ttbl48aIpk+osF1d4A2xFdEZA00DStGlTGTJkiPnGrM8L27x5s7mI6mN5+k6sXr1aunTpYi4u+vj6668X+97F9UH55JNPZNCgQeYbuzY39OvXT3bs2HFZxxgVFSUrVqwwF+Lf/OY37vm637oPehxlHa/WJOix7NmzR3r37m2CyWOPPWaWaQ2Dll9cXJy5oLVr106eeOIJyc/P93r92rVrTe2EqznF1SenpHLcuHGj3HTTTdKwYUPzzX748OFy8OBBr3VcfTK05stVAxARESE/+9nPTIC6HNOnT5cBAwbIypUr5V//+lepfVCeffZZueqqq0y56PnUvXt3U2Pk2sdHHnnE/N2mTRv38bvKXf+eNGmSOfd0G1qG69atK7WfkvZBufPOO815op+vNuNduHChXP16PLdZ1r4V1wfliy++kJ/85CemCVGP94YbbjCfbXHn0KuvvmrOuZYtW5r/H/R81s8K8AVqUBDQ9KJw++23m2/FI0eOlEWLFsmuXbvkuuuuq9T2tPo/OTlZOnfuLOnp6fLdd9+Zi6X+A12WAwcOmAuyXnQeffRR0+Txxz/+0VwMt2zZcll9ZFq1aiU333yzbNq0SbKzs72aLMpLj0XD01133WVqHFq0aGHm60VQw5Q2H+mjBosZM2aY95k/f75Z51e/+pVpcvrqq6/kmWeeMfN03ZK899575r20P4heRLWpQ0NAz5495eOPPy7S4Vgv1nqB1TLX5dpkEx0dbZq1Lsc999xjPtP169eb2qniPP/88/LQQw+ZgOsKCvv27ZOdO3fKqFGjzPmlAefll182x96sWTPzuubNm7u3oWWmF3MNKrq88PEVpser6+jxaoD9/e9/L//5z3/kpZdeqtDxlWffPJ06dUpuvPFGE/70mDUcaRPisGHD5LXXXpMf//jHXutrLZQ2ET388MPm8583b56MHj3alA1w2RwgQO3evdvRU3z9+vXmeUFBgdOyZUvnF7/4hdd6mzZtMuvpo6cjR46Y+UuWLHHPu+aaa5zY2FjnzJkz7nnvvvuuWa9169Zer9d5M2fOdD8fMWKEExoa6hw+fNg97+uvv3YaN27s9O7du8zj0e1NnDixxOV6XLrOp59+ap7rfutzPY6yjvfmm2828xYvXlxku+fPny8yb/z48U6DBg2cCxcuuOcNGTKkSBmUVo7R0dHOd999556n+x0cHOzce++97nlafvra++67z2ubP/7xj52oqCinLCkpKU7Dhg1LXP7JJ5+Y7U+dOtWrLHRyGT58uHPVVVeV+j7z588vtqyVztfjOnDgQLHLPM8R1/EOGzbMa70HH3zQ67MtrkxL2mZp+6afl5aRy5QpU8y677//vnve2bNnnTZt2jgJCQlOfn6+1znUqVMnJzc3173u//7v/5r5n332WanlBZQHTTwI6NoTrQXo27evea5V0j/96U9Nc4hn80R5nThxQvbu3SspKSmmmcGz46rWqJRG30+/qY8YMcLUGrjExsaab+EffPCBqZG4HK4ai7Nnz1bq9dr0oLVBhdWvX9/9t25bmx+0Jki/ZX/++eeVLkdtWtBmBJfExERTlm+//XaR10yYMMHrub6/1vhURZlps5LWDGnNW2Vp7VZZ54iniRMnej2fPHmyeSyubHxJt3/99ddLr169vMro/vvvN81ChUeL6fni2WdHPxdXMxFwuQgoCEgaCDSIaDjRjrLaLq6TNqNoNfaGDRsqvE3tX6GuvPLKIss6dOhQ6mu/+eYbc0Evbr1OnTqZjpPHjx+Xy3Hu3Dnz2Lhx40q9/oorrii2g6g2TWnVvoYybTrS5gFXp1Ot1q9sOZZUFhqAcnJyijRhedJ+IEqbPfxdZtOmTTMXab1w62ev4eHDDz+s0Pto81RFFD7HtN+PNqUU7k/ka/rZlPS5uJZXxecCKAIKApK2+es3dQ0p+o+9a9K2feXZWVZrVopTmVqW6qRDfOvUqeO+GFb0uDxrSlz03hn67V+HM8+ZM8cMz9X+Gq6+H766j0hZ9LiK8/9bNC6vzFRpQ8/14qzDcfVc0poFvX+KPlZkNFhxZVsRhT9LW85Zf30ugKKTLAKSBhDtRLlw4cIiy3TIsY68Wbx4sblwuL71Fb6RVeFvi3o/D3Xo0KEi29QLWGm01kFHRBS3njaT6Lfj+Ph4qaxjx46ZjrY9evRw1waU97hKo6M1tClFy0xH97horVRhJV00C3OVY0lloR05dWRPVfjrX/9q9lublkqj+6PNgzrpEGHtfKqjV3SYso5eKe+xl5eeY561Llr7p2HQ1bm2Ip9tRfZNP5uSPhfXcqCqUIOCgKMjQvSC+qMf/ciMvCg86UgK7XPw5ptvuv/R1W+CW7du9dqO3vzMk/YXueaaa8yoBs+mDa1RKOtOrrp9HdKqQ3Y9q+m1uUmHq+o38sqMvFHff/+9GaGk3551NI1ns4DyPC5d509/+lOFvyF7fiPWC3ThsnFdxMvT5ONZjp4XWK3N0H46vrqPS1l0BIq+n4aO4prtXDSgedJmMO1PomWSl5dn5rkCla/u1lo4WOsIJ6Ujn5SeKxrkyjpnK7pvWvZ6Q8Pt27e752lzm54zGo4q0o8GuFzUoCDgaPDQAKJDI4uj93XQGg2tZdGLk/at0Ps+6EVAv23qhX3NmjVy+vTpIq/VYZ96TxANFPfdd58JB657ZLj6M5REbw6mYUZf++CDD5r7lugw49zcXDM8szx0yOjf/vY3c3HUDqLa9KL38dD3fvrpp+W2225zr6v7pMeq3/J1P7VDqjZTXLp0ScpLh5zqt3XtGKzDTrV8tNahuCp8ve2+3vBMhyPrMG7tt6E3kCuODk/Wi63W+IwdO9Y9zFg/C1//fpEer5aZ0iHCWsug54gOFdY+SmUFNg2WMTExZgi0drrWe7X84Q9/MOeBq7ZKj11pQNRh2jqEXI+9sjVBWkOl569+nhoWdP+1M3XXrl3d6/z85z83IUsf9b4sGlY87+fiUpF903vD6JBk/Wz089ZzRoOk7o82bXHXWVSpco31AWqQoUOHOvXq1XNycnJKXGfMmDFO3bp1nW+//dY8/+abb5zk5GQzdLZp06ZmGO3+/fuLHcr597//3QyvDAsLczp37uysWrXKDNUsa5ix+vjjj52BAwc6jRo1Mu/Vt29fZ9u2beU6Lt2ea9Jhq02aNHGuvfZaM7y4uCGsSoc09+/f3+xrixYtnMcee8wMuy5umHFJQ2k//PBD54YbbnDq16/vxMXFOY8++qjzj3/8o8g2zp0754waNcrsl+ew65KGxL733ntOz549zXbDw8PN5/bPf/7Tax3XsFv9fDyVNIS6MP1cPMtNy1yHy+pn/dprr7mHzXoqPMz4j3/8oxkGrsOatRzbtWvnPPLII05WVpbX65544gnniiuuMJ+N576VNjy8pGHGWg533HGHGYKu5+OkSZOc//73v0WGf48dO9aJiIgw6915553O6dOniz3vStq3wsOMXeeMvrd+jvr/0fXXX++sWbPGax3XMOOVK1d6zS9t+DNQUUH6n6qNRAAAAKWjDwoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHVq5I3a9JbPX3/9tblJkq9vMQ0AAPxD72yiN9KMi4sr88Z/NTKgaDi5nN8tAQAA1Ud/vb1ly5aBF1Bct5fWA6zs75cAAICqpT/RoRUMrut4wAUUV7OOhhMCCgAANUt5umfQSRYAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAAanZASU9Pl+uuu87cQz86OlpGjBghGRkZXutcuHBBJk6cKFFRUdKoUSNJTk6WU6dOea1z7NgxGTJkiDRo0MBs55FHHpFLly755ogAAEDtCihbtmwx4WPHjh2yfv16ycvLkwEDBkhOTo57nalTp8pbb70lK1euNOvrLw/ffvvt7uX5+fkmnFy8eFG2bdsmL774oixdulRmzJjh2yMDAAA1VpDjOE5lX/zNN9+YGhANIr1795asrCxp3ry5LF++XO644w6zzueffy6dOnWS7du3yw033CDvvPOO/OhHPzLBpUWLFmadxYsXy7Rp08z2QkNDy/VriBEREeb9+LFAAABqhopcvy+rD4q+gYqMjDSPe/bsMbUq/fv3d6/TsWNHadWqlQkoSh+vvvpqdzhRAwcONDt94MCBYt8nNzfXLPecAABA4Aqp7AsLCgpkypQp0rNnT+nSpYuZd/LkSVMD0qRJE691NYzoMtc6nuHEtdy1rKS+L7Nnz67srgKoYRKmry33ukfnDvHrvgCoHpWuQdG+KPv375cVK1aIv6WlpZnaGtd0/Phxv78nAACoYTUokyZNkjVr1sjWrVulZcuW7vkxMTGm8+uZM2e8alF0FI8uc63z0UcfeW3PNcrHtU5hYWFhZgIAALVDhWpQtD+thpPXX39dNm7cKG3atPFa3q1bN6lbt65s2LDBPU+HIeuw4h49epjn+vjZZ5/J6dOn3evoiCDtLNO5c+fLPyIAAFC7alC0WUdH6LzxxhvmXiiuPiPaI7d+/frmcezYsZKammo6zmromDx5sgklOoJH6bBkDSL33HOPzJs3z2zj8ccfN9umlgQAAFQ4oCxatMg89unTx2v+kiVLZMyYMebvZ555RoKDg80N2nT0jY7Qee6559zr1qlTxzQPPfDAAya4NGzYUFJSUmTOnDl8IgAA4PLvg1JduA8KENgYxQMEpiq7DwoAAIA/EFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAgJofULZu3SpDhw6VuLg4CQoKktWrV3st13nFTfPnz3evk5CQUGT53LlzfXNEAACg9gWUnJwc6dq1qyxcuLDY5SdOnPCa/vKXv5gAkpyc7LXenDlzvNabPHly5Y8CAAAElJCKvmDQoEFmKklMTIzX8zfeeEP69u0rbdu29ZrfuHHjIusCAAD4vQ/KqVOnZO3atTJ27Ngiy7RJJyoqSq699lrT/HPp0qUSt5ObmyvZ2dleEwAACFwVrkGpiBdffNHUlNx+++1e8x966CH54Q9/KJGRkbJt2zZJS0szzTxPP/10sdtJT0+X2bNn+3NXAQBAbQko2v9k9OjRUq9ePa/5qamp7r8TExMlNDRUxo8fb4JIWFhYke1ogPF8jdagxMfH+3PXAQBAIAaU999/XzIyMuSVV14pc92kpCTTxHP06FHp0KFDkeUaWooLLgAAIDD5rQ/KCy+8IN26dTMjfsqyd+9eCQ4OlujoaH/tDgAACOQalHPnzklmZqb7+ZEjR0zA0P4krVq1cjfBrFy5Un73u98Vef327dtl586dZmSP9k/R51OnTpW7775bmjZternHAwAAamNA2b17twkXLq6+ISkpKbJ06VLz94oVK8RxHBk5cmSR12tTjS6fNWuWGZ3Tpk0bE1A8+5gAAIDaLcjRJFHDaA1NRESEZGVlSXh4eHXvDgAfS5i+ttzrHp07hPIHAvD6zW/xAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAICaH1C2bt0qQ4cOlbi4OAkKCpLVq1d7LR8zZoyZ7znddtttXut8//33Mnr0aAkPD5cmTZrI2LFj5dy5c5d/NAAAoHYGlJycHOnatassXLiwxHU0kJw4ccI9vfzyy17LNZwcOHBA1q9fL2vWrDGh5/7776/cEQAAgIATUtEXDBo0yEylCQsLk5iYmGKXHTx4UNatWye7du2S7t27m3nPPvusDB48WJ566ilTMwMAAGo3v/RB2bx5s0RHR0uHDh3kgQcekO+++869bPv27aZZxxVOVP/+/SU4OFh27txZ7PZyc3MlOzvbawIAAIHL5wFFm3deeukl2bBhg/z2t7+VLVu2mBqX/Px8s/zkyZMmvHgKCQmRyMhIs6w46enpEhER4Z7i4+N9vdsAAKAmN/GU5a677nL/ffXVV0tiYqK0a9fO1Kr069evUttMS0uT1NRU93OtQSGkAAAQuPw+zLht27bSrFkzyczMNM+1b8rp06e91rl06ZIZ2VNSvxXt06IjfjwnAAAQuPweUL766ivTByU2NtY879Gjh5w5c0b27NnjXmfjxo1SUFAgSUlJ/t4dAAAQiE08er8SV22IOnLkiOzdu9f0IdFp9uzZkpycbGpDDh8+LI8++qj84Ac/kIEDB5r1O3XqZPqpjBs3ThYvXix5eXkyadIk0zTECB4AAFCpGpTdu3fLtddeayalfUP07xkzZkidOnVk3759MmzYMGnfvr25AVu3bt3k/fffN800LsuWLZOOHTuaPik6vLhXr17ypz/9iU8EAABUrgalT58+4jhOicv/8Y9/lLkNrWlZvnx5Rd8aAADUEvwWDwAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACo+QFl69atMnToUImLi5OgoCBZvXq1e1leXp5MmzZNrr76amnYsKFZ595775Wvv/7aaxsJCQnmtZ7T3LlzfXNEAACg9gWUnJwc6dq1qyxcuLDIsvPnz8vHH38sv/71r83jqlWrJCMjQ4YNG1Zk3Tlz5siJEyfc0+TJkyt/FAAAIKCEVPQFgwYNMlNxIiIiZP369V7z/vCHP8j1118vx44dk1atWrnnN27cWGJiYiqzzwAAIMD5vQ9KVlaWacJp0qSJ13xt0omKipJrr71W5s+fL5cuXSpxG7m5uZKdne01AQCAwFXhGpSKuHDhgumTMnLkSAkPD3fPf+ihh+SHP/yhREZGyrZt2yQtLc008zz99NPFbic9PV1mz57tz10FAAC1IaBoh9k777xTHMeRRYsWeS1LTU11/52YmCihoaEyfvx4E0TCwsKKbEsDjOdrtAYlPj7eX7sOAAACMaC4wsmXX34pGzdu9Ko9KU5SUpJp4jl69Kh06NChyHINLcUFFwAAEJhC/BVODh06JJs2bTL9TMqyd+9eCQ4OlujoaF/vDgAAqA0B5dy5c5KZmel+fuTIERMwtD9JbGys3HHHHWaI8Zo1ayQ/P19Onjxp1tPl2pSzfft22blzp/Tt29eM5NHnU6dOlbvvvluaNm3q26MDAAC1I6Ds3r3bhAsXV9+QlJQUmTVrlrz55pvm+TXXXOP1Oq1N6dOnj2mqWbFihVlXR+e0adPGBBTPPiYAAKB2q3BA0ZChHV9LUtoypaN3duzYUdG3BQAAtQi/xQMAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAAan5A2bp1qwwdOlTi4uIkKChIVq9e7bXccRyZMWOGxMbGSv369aV///5y6NAhr3W+//57GT16tISHh0uTJk1k7Nixcu7cucs/GgAAUDsDSk5OjnTt2lUWLlxY7PJ58+bJ73//e1m8eLHs3LlTGjZsKAMHDpQLFy6419FwcuDAAVm/fr2sWbPGhJ7777//8o4EAAAEjJCKvmDQoEFmKo7WnixYsEAef/xxGT58uJn30ksvSYsWLUxNy1133SUHDx6UdevWya5du6R79+5mnWeffVYGDx4sTz31lKmZAQAAtZtP+6AcOXJETp48aZp1XCIiIiQpKUm2b99unuujNuu4wonS9YODg02NS3Fyc3MlOzvbawIAAIHLpwFFw4nSGhNP+ty1TB+jo6O9loeEhEhkZKR7ncLS09NN0HFN8fHxvtxtAABgmRoxiictLU2ysrLc0/Hjx6t7lwAAQE0JKDExMebx1KlTXvP1uWuZPp4+fdpr+aVLl8zIHtc6hYWFhZkRP54TAAAIXD4NKG3atDEhY8OGDe552l9E+5b06NHDPNfHM2fOyJ49e9zrbNy4UQoKCkxfFQAAgAqP4tH7lWRmZnp1jN27d6/pQ9KqVSuZMmWKPPnkk3LllVeawPLrX//ajMwZMWKEWb9Tp05y2223ybhx48xQ5Ly8PJk0aZIZ4cMIHgAAUKmAsnv3bunbt6/7eWpqqnlMSUmRpUuXyqOPPmrulaL3NdGakl69eplhxfXq1XO/ZtmyZSaU9OvXz4zeSU5ONvdOAQAAUEGO3rykhtFmIx3Nox1m6Y8CBJ6E6WvLve7RuUP8ui8Aquf6XSNG8QAAgNqFgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAABH5ASUhIkKCgoCLTxIkTzfI+ffoUWTZhwgRf7wYAAKjBQny9wV27dkl+fr77+f79++XWW2+Vn/zkJ+5548aNkzlz5rifN2jQwNe7AQAAajCfB5TmzZt7PZ87d660a9dObr75Zq9AEhMT4+u3BgAAAcKvfVAuXrwof/vb3+S+++4zTTkuy5Ytk2bNmkmXLl0kLS1Nzp8/X+p2cnNzJTs722sCAACBy+c1KJ5Wr14tZ86ckTFjxrjnjRo1Slq3bi1xcXGyb98+mTZtmmRkZMiqVatK3E56errMnj3bn7sKAAAsEuQ4juOvjQ8cOFBCQ0PlrbfeKnGdjRs3Sr9+/SQzM9M0BZVUg6KTi9agxMfHS1ZWloSHh/tl3wFUn4Tpa8u97tG5Q/y6LwB8R6/fERER5bp++60G5csvv5T33nuv1JoRlZSUZB5LCyhhYWFmAgAAtYPf+qAsWbJEoqOjZciQ0r/d7N271zzGxsb6a1cAAEAN45calIKCAhNQUlJSJCTk/97i8OHDsnz5chk8eLBERUWZPihTp06V3r17S2Jioj92BQAA1EB+CSjatHPs2DEzeseT9kfRZQsWLJCcnBzTjyQ5OVkef/xxf+wGAACoofwSUAYMGCDF9b3VQLJlyxZ/vCUAAAgg/BYPAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAAAj8gDJr1iwJCgrymjp27OhefuHCBZk4caJERUVJo0aNJDk5WU6dOuXr3QAAADWYX2pQrrrqKjlx4oR7+uCDD9zLpk6dKm+99ZasXLlStmzZIl9//bXcfvvt/tgNAABQQ4X4ZaMhIRITE1NkflZWlrzwwguyfPlyueWWW8y8JUuWSKdOnWTHjh1yww03+GN3AABADeOXGpRDhw5JXFyctG3bVkaPHi3Hjh0z8/fs2SN5eXnSv39/97ra/NOqVSvZvn17idvLzc2V7OxsrwkAAAQunweUpKQkWbp0qaxbt04WLVokR44ckZtuuknOnj0rJ0+elNDQUGnSpInXa1q0aGGWlSQ9PV0iIiLcU3x8vK93GwAABHITz6BBg9x/JyYmmsDSunVrefXVV6V+/fqV2mZaWpqkpqa6n2sNCiEFAIDA5fdhxlpb0r59e8nMzDT9Ui5evChnzpzxWkdH8RTXZ8UlLCxMwsPDvSYAABC4/B5Qzp07J4cPH5bY2Fjp1q2b1K1bVzZs2OBenpGRYfqo9OjRw9+7AgAAamsTz8MPPyxDhw41zTo6hHjmzJlSp04dGTlypOk/MnbsWNNcExkZaWpCJk+ebMIJI3gAAIDfAspXX31lwsh3330nzZs3l169epkhxPq3euaZZyQ4ONjcoE1H5wwcOFCee+45X+8GAACowYIcx3GkhtFOslobo/dVoT8KEHgSpq8t97pH5w7x674AqJ7rN7/FAwAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAAACP6Ckp6fLddddJ40bN5bo6GgZMWKEZGRkeK3Tp08fCQoK8pomTJjg610BAAA1lM8DypYtW2TixImyY8cOWb9+veTl5cmAAQMkJyfHa71x48bJiRMn3NO8efN8vSsAAKCGCvH1BtetW+f1fOnSpaYmZc+ePdK7d2/3/AYNGkhMTIyv3x4AAAQAv/dBycrKMo+RkZFe85ctWybNmjWTLl26SFpampw/f77EbeTm5kp2drbXBAAAApfPa1A8FRQUyJQpU6Rnz54miLiMGjVKWrduLXFxcbJv3z6ZNm2a6aeyatWqEvu1zJ4925+7CgAALBLkOI7jr40/8MAD8s4778gHH3wgLVu2LHG9jRs3Sr9+/SQzM1PatWtXbA2KTi5agxIfH29qZ8LDw/21+wCqScL0teVe9+jcIX7dFwC+o9fviIiIcl2//VaDMmnSJFmzZo1s3bq11HCikpKSzGNJASUsLMxMAACgdvB5QNEKmcmTJ8vrr78umzdvljZt2pT5mr1795rH2NhYX+8OAACogXweUHSI8fLly+WNN94w90I5efKkma9VOvXr15fDhw+b5YMHD5aoqCjTB2Xq1KlmhE9iYqKvdwcAANRAPg8oixYtct+MzdOSJUtkzJgxEhoaKu+9954sWLDA3BtF+5IkJyfL448/7utdAQAANZRfmnhKo4FEb+YGAABQEn6LBwAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYp1oDysKFCyUhIUHq1asnSUlJ8tFHH1Xn7gAAgNoeUF555RVJTU2VmTNnyscffyxdu3aVgQMHyunTp6trlwAAQG0PKE8//bSMGzdOfvazn0nnzp1l8eLF0qBBA/nLX/5SXbsEAAAsEVIdb3rx4kXZs2ePpKWluecFBwdL//79Zfv27UXWz83NNZNLVlaWeczOzq6iPQZQlQpyz5d7Xf4dAGoO1/+vjuPYGVC+/fZbyc/PlxYtWnjN1+eff/55kfXT09Nl9uzZRebHx8f7dT8B2C9iQXXvAYCKOnv2rERERNgXUCpKa1q0v4pLQUGBfP/99xIVFSVBQUFS22ki1bB2/PhxCQ8Pr+7dCViUM+UcSDifKefqoDUnGk7i4uLKXLdaAkqzZs2kTp06curUKa/5+jwmJqbI+mFhYWby1KRJE7/vZ02j4YSAQjkHCs5nyjmQcD7/n7JqTqq1k2xoaKh069ZNNmzY4FUros979OhRHbsEAAAsUm1NPNpkk5KSIt27d5frr79eFixYIDk5OWZUDwAAqN2qLaD89Kc/lW+++UZmzJghJ0+elGuuuUbWrVtXpOMsyqbNX3o/mcLNYPAtyrlqUM6UcyDhfK68IKc8Y30AAACqEL/FAwAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUy+nvEF133XXSuHFjiY6OlhEjRkhGRkaZrztz5oxMnDhRYmNjzTC39u3by9tvv10l+1ybylnv39OhQwepX7+++bmBqVOnyoULF6pkn2uiRYsWSWJiovuumnpjxnfeeafU16xcuVI6duwo9erVk6uvvprz2A/l/Pzzz8tNN90kTZs2NZP+cOtHH31U8Q+4FqrMOe2yYsUK83Mt+u8NiiKgWG7Lli0maOzYsUPWr18veXl5MmDAAHNTu9J+LfrWW2+Vo0ePymuvvWYutPoP0BVXXFGl+x7o5bx8+XKZPn26uQfNwYMH5YUXXpBXXnlFHnvssSrd95qkZcuWMnfuXPNr5rt375ZbbrlFhg8fLgcOHCh2/W3btsnIkSNl7Nix8sknn5h/yHXav39/le97IJfz5s2bTTlv2rTJ/KK8hm09///9739X+b4Helm76L/PDz/8sAmGKIHeBwU1x+nTp/W+Nc6WLVtKXGfRokVO27ZtnYsXL1bpvtW2cp44caJzyy23eM1LTU11evbsWQV7GDiaNm3q/PnPfy522Z133ukMGTLEa15SUpIzfvz4Ktq72lHOhV26dMlp3Lix8+KLL/p9v2pjWWv53njjjWadlJQUZ/jw4VW6fzUFNSg1TFZWlnmMjIwscZ0333zTVDNqjYDembdLly7yP//zP5Kfn1+Fexr45XzjjTeab02uqvAvvvjCND8MHjy4yvazJtPzUau4tZaqpN/g0m/z2tzgaeDAgWY+fFfOhZ0/f97UIpZ2/qPyZT1nzhzTlKw1g7DwVveoOP1BxSlTpkjPnj1N6CiJXig3btwoo0ePNhfMzMxMefDBB80/ONocAd+U86hRo+Tbb7+VXr16mZ8Qv3TpkkyYMIEmnjJ89tln5h9v7avTqFEjef3116Vz587Frqs/g1H45y/0uc6H78q5sGnTpklcXFyRcIjLL+sPPvjANAfv3buX4ixLdVfhoPwmTJjgtG7d2jl+/Hip61155ZVOfHy8qUZ0+d3vfufExMRQ3D4s502bNjktWrRwnn/+eWffvn3OqlWrTLnPmTOHci5Fbm6uc+jQIWf37t3O9OnTnWbNmjkHDhwodt26des6y5cv95q3cOFCJzo6mjL2YTl7Sk9PN00Un376KWXs47LOzs52EhISnLfffts9jyaekhFQagjt79CyZUvniy++KHPd3r17O/369fOap/9DaB7V/5Hgm3Lu1auX8/DDD3vN++tf/+rUr1/fyc/Pp5jLSc/V+++/v9hlGvieeeYZr3kzZsxwEhMTKV8flrPL/PnznYiICGfXrl2Urx/K+pNPPjH/DtepU8c9BQUFmUn/zszMpNw90AfFchoiJ02aZKoMtdmmTZs2Zb5Gmya0WUebKlz+9a9/mSHHoaGhft7j2lPO2k4fHOz9v1CdOnXc20P56Hmam5tb7DKtNt+wYYPXPB1lVd6+FChfOat58+bJE088YX5Vvnv37hSdH8pah8trc5A277imYcOGSd++fc3fOnoKHjzTCuzzwAMPmG80mzdvdk6cOOGezp8/717nnnvuMdWKLseOHTM98CdNmuRkZGQ4a9asMVXiTz75ZDUdRWCW88yZM005v/zyy6bG5d1333XatWtnRp6geFp+OjLqyJEjpllMn+u3Ry274sr4ww8/dEJCQpynnnrKOXjwoClzbfb57LPPKGIflvPcuXOd0NBQ57XXXvM6/8+ePUs5+7isC6OJp2QEFMtphixuWrJkiXudm2++2ZzknrZt22aGY4aFhZkhx7/5zW+8+qTg8ss5Ly/PmTVrlgkl9erVM80RDz74oPOf//yH4i3BfffdZ/r36MWwefPmpirc9Q95Sefyq6++6rRv39685qqrrnLWrl1L+fq4nHXd4s5/DYTwbVkXRkApWZD+x7NGBQAAoLrRBwUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAAYpv/Bx7QsQ++3s0tAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram Helpers"
   ],
   "id": "828f033f4bd61935"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.496134Z",
     "start_time": "2025-12-17T16:51:43.494038Z"
    }
   },
   "source": [
    "def load_audio(path, sr):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    return y\n",
    "\n",
    "def make_mel(y, sr):\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_mels=args.n_mels\n",
    "    )\n",
    "    return librosa.power_to_db(S, ref=np.max)"
   ],
   "id": "e871cdb5d7496057",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.504177Z",
     "start_time": "2025-12-17T16:51:43.502273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_noise(y, snr_db=10):\n",
    "    signal_power = np.mean(y**2)\n",
    "    noise_power = signal_power / (10**(snr_db / 10))\n",
    "    noise = np.random.normal(0, np.sqrt(noise_power), size=y.shape)\n",
    "    return y + noise"
   ],
   "id": "f1fa1fdc45cdce4e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ],
   "id": "9e3261affa75fdf3"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.511632Z",
     "start_time": "2025-12-17T16:51:43.507806Z"
    }
   },
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, pairs, fixed_length=200, augment=False):\n",
    "        self.pairs = pairs\n",
    "        self.fixed_length = fixed_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.pairs[idx]\n",
    "        y = load_audio(path, args.sr)\n",
    "\n",
    "        if self.augment:\n",
    "            y = self.add_noise(y)\n",
    "            y = self.pitch_shift(y)\n",
    "            y = self.time_stretch(y)\n",
    "\n",
    "        S = make_mel(y, args.sr)\n",
    "        S = (S - S.mean()) / (S.std() + 1e-9)\n",
    "\n",
    "        if S.shape[1] < self.fixed_length:\n",
    "            pad_width = self.fixed_length - S.shape[1]\n",
    "            S = np.pad(S, ((0,0),(0,pad_width)), mode='constant')\n",
    "        else:\n",
    "            S = S[:, :self.fixed_length]\n",
    "\n",
    "        return torch.tensor(S).unsqueeze(0).float(), torch.tensor(label).long()\n",
    "\n",
    "    def add_noise(self, y, snr_db=10):\n",
    "        rms_signal = np.sqrt(np.mean(y**2))\n",
    "        rms_noise = rms_signal / (10**(snr_db/20))\n",
    "        noise = np.random.normal(0, rms_noise, y.shape[0])\n",
    "        return y + noise\n",
    "\n",
    "    def pitch_shift(self, y, n_steps=1):\n",
    "        return librosa.effects.pitch_shift(y, sr=args.sr, n_steps=random.uniform(-n_steps, n_steps))\n",
    "\n",
    "    def time_stretch(self, y, rate_range=(0.9, 1.1)):\n",
    "        rate = random.uniform(*rate_range)\n",
    "        try:\n",
    "            return librosa.effects.time_stretch(y, rate)\n",
    "        except:\n",
    "            return y"
   ],
   "id": "9fa0c8b8319a4fbc",
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train / Val / Test Split"
   ],
   "metadata": {},
   "id": "a08b33da28499752"
  },
  {
   "cell_type": "code",
   "source": [
    "paths = [p for p, _ in pairs]\n",
    "labels = [l for _, l in pairs]\n",
    "\n",
    "tr_p, te_p, tr_l, te_l = train_test_split(\n",
    "    paths, labels,\n",
    "    test_size=args.test_size,\n",
    "    stratify=labels,\n",
    "    random_state=args.seed\n",
    ")\n",
    "\n",
    "tr_p, va_p, tr_l, va_l = train_test_split(\n",
    "    tr_p, tr_l,\n",
    "    test_size=args.val_size,\n",
    "    stratify=tr_l,\n",
    "    random_state=args.seed\n",
    ")\n",
    "\n",
    "train_pairs = list(zip(tr_p, tr_l))\n",
    "val_pairs   = list(zip(va_p, va_l))\n",
    "test_pairs  = list(zip(te_p, te_l))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    SpectrogramDataset(train_pairs, augment=True),\n",
    "    batch_size=args.batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    SpectrogramDataset(val_pairs, augment=False),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    SpectrogramDataset(test_pairs, augment=False),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "\n",
    "print(\"Train / Val / Test loaders created:\")\n",
    "print(f\"Train: {len(train_pairs)}, Val: {len(val_pairs)}, Test: {len(test_pairs)}\")\n",
    "\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.554717Z",
     "start_time": "2025-12-17T16:51:43.515095Z"
    }
   },
   "id": "7f907e2ba8464141",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Val / Test loaders created:\n",
      "Train: 29724, Val: 5246, Test: 6172\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": "## Advanced Model",
   "metadata": {},
   "id": "81faefab744bcaa6"
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self, fixed_length=200):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.c2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, args.n_mels, fixed_length)\n",
    "            x = self.pool(F.relu(self.c1(dummy)))\n",
    "            x = self.pool(F.relu(self.c2(x)))\n",
    "            self.flattened = x.numel()\n",
    "\n",
    "        self.fc = nn.Linear(self.flattened, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.c1(x)))\n",
    "        x = self.pool(F.relu(self.c2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.566723Z",
     "start_time": "2025-12-17T16:51:43.563378Z"
    }
   },
   "id": "a87a5b8089d04ba8",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T16:51:43.571854Z",
     "start_time": "2025-12-17T16:51:43.569351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            out = model(X)\n",
    "            preds.extend(out.argmax(1).tolist())\n",
    "            labels.extend(y.tolist())\n",
    "\n",
    "    return f1_score(labels, preds, average=\"macro\")\n"
   ],
   "id": "ea9f18992f6c52fc",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Infinite Training Loop (Interrupt Kernel to Stop)"
   ],
   "metadata": {},
   "id": "44257225a6adeb5c"
  },
  {
   "cell_type": "code",
   "source": [
    "ensure_dir(\"./models\")\n",
    "ensure_dir(\"./outputs\")\n",
    "\n",
    "model = SimpleConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "loss_history = []\n",
    "train_f1_history = []\n",
    "val_f1_history = []\n",
    "real_test_f1_history = []\n",
    "\n",
    "epoch = 1\n",
    "plt.ioff()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        model.train()\n",
    "        losses = []\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "\n",
    "        for X, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            train_preds.extend(out.argmax(1).tolist())\n",
    "            train_labels.extend(y.tolist())\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        train_f1 = f1_score(train_labels, train_preds, average=\"macro\")\n",
    "\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                out = model(X)\n",
    "                val_preds.extend(out.argmax(1).tolist())\n",
    "                val_labels.extend(y.tolist())\n",
    "        val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "\n",
    "        real_preds, real_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                out = model(X)\n",
    "                real_preds.extend(out.argmax(1).tolist())\n",
    "                real_labels.extend(y.tolist())\n",
    "        real_test_f1 = f1_score(real_labels, real_preds, average=\"macro\")\n",
    "\n",
    "        loss_history.append(avg_loss)\n",
    "        train_f1_history.append(train_f1)\n",
    "        val_f1_history.append(val_f1)\n",
    "        real_test_f1_history.append(real_test_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Loss {avg_loss:.4f} | Train F1 {train_f1:.3f} | \"\n",
    "              f\"Val F1 {val_f1:.3f} | Real Test F1 {real_test_f1:.3f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"./models/model_epoch_{epoch}.pth\")\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_title(\"Training Curves\")\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.plot(loss_history, label=\"Loss\", color='blue')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel(\"F1 Score\")\n",
    "        ax2.plot(train_f1_history, label=\"Train F1\", color='green', linestyle='-')\n",
    "        ax2.plot(val_f1_history, label=\"Val F1\", color='orange', linestyle='--')\n",
    "        ax2.plot(real_test_f1_history, label=\"Real Test F1\", color='red', linestyle=':')\n",
    "\n",
    "        fig.legend(loc=\"upper left\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"./outputs/epoch_{epoch:03d}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user.\")\n",
    "        break\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T19:50:32.089039Z",
     "start_time": "2025-12-17T17:05:30.440455Z"
    }
   },
   "id": "2960badb0b31f5ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.4252 | Train F1 0.809 | Val F1 0.830 | Real Test F1 0.833\n",
      "Epoch 2 | Loss 0.3467 | Train F1 0.847 | Val F1 0.775 | Real Test F1 0.764\n",
      "Epoch 3 | Loss 0.3212 | Train F1 0.859 | Val F1 0.854 | Real Test F1 0.854\n",
      "Epoch 4 | Loss 0.3046 | Train F1 0.869 | Val F1 0.843 | Real Test F1 0.842\n",
      "Epoch 5 | Loss 0.2944 | Train F1 0.875 | Val F1 0.856 | Real Test F1 0.860\n",
      "Epoch 6 | Loss 0.2870 | Train F1 0.878 | Val F1 0.833 | Real Test F1 0.839\n",
      "Epoch 7 | Loss 0.2790 | Train F1 0.880 | Val F1 0.855 | Real Test F1 0.861\n",
      "Epoch 8 | Loss 0.2719 | Train F1 0.884 | Val F1 0.823 | Real Test F1 0.815\n",
      "Epoch 9 | Loss 0.2653 | Train F1 0.885 | Val F1 0.864 | Real Test F1 0.872\n",
      "Epoch 10 | Loss 0.2585 | Train F1 0.892 | Val F1 0.869 | Real Test F1 0.871\n",
      "Epoch 11 | Loss 0.2598 | Train F1 0.889 | Val F1 0.873 | Real Test F1 0.877\n",
      "Epoch 12 | Loss 0.2504 | Train F1 0.894 | Val F1 0.868 | Real Test F1 0.873\n",
      "Epoch 13 | Loss 0.2538 | Train F1 0.894 | Val F1 0.867 | Real Test F1 0.870\n",
      "Epoch 14 | Loss 0.2484 | Train F1 0.896 | Val F1 0.869 | Real Test F1 0.873\n",
      "Epoch 15 | Loss 0.2492 | Train F1 0.895 | Val F1 0.858 | Real Test F1 0.863\n",
      "Epoch 16 | Loss 0.2460 | Train F1 0.896 | Val F1 0.861 | Real Test F1 0.859\n",
      "Epoch 17 | Loss 0.2430 | Train F1 0.898 | Val F1 0.866 | Real Test F1 0.867\n",
      "Epoch 18 | Loss 0.2418 | Train F1 0.899 | Val F1 0.849 | Real Test F1 0.850\n",
      "Epoch 19 | Loss 0.2422 | Train F1 0.899 | Val F1 0.880 | Real Test F1 0.884\n",
      "Epoch 20 | Loss 0.2378 | Train F1 0.898 | Val F1 0.876 | Real Test F1 0.887\n",
      "Training interrupted by user.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T17:05:24.207098Z",
     "start_time": "2025-12-17T17:05:24.105740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SimpleConvNet()\n",
    "model.load_state_dict(torch.load(\"./models/model_epoch_XX.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def preprocess_audio(path):\n",
    "    y, _ = librosa.load(path, sr=args.sr)\n",
    "    S = make_mel(y)\n",
    "    S = (S - S.mean()) / (S.std() + 1e-9)\n",
    "\n",
    "    if S.shape[1] < args.fixed_length:\n",
    "        S = np.pad(S, ((0, 0), (0, args.fixed_length - S.shape[1])))\n",
    "    else:\n",
    "        S = S[:, :args.fixed_length]\n",
    "\n",
    "    return torch.tensor(S).unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "\n",
    "X = preprocess_audio(\"my_voice.wav\")\n",
    "with torch.no_grad():\n",
    "    pred = model(X).argmax(1).item()\n",
    "\n",
    "print(\"ALLOWED\" if pred == 1 else \"NOT ALLOWED\")"
   ],
   "id": "80b15bad09a37f47",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/model_epoch_XX.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m model = SimpleConvNet()\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m model.load_state_dict(\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./models/model_epoch_XX.pth\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[32m      3\u001B[39m model.eval()\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpreprocess_audio\u001B[39m(path):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\serialization.py:1484\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1481\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args.keys():\n\u001B[32m   1482\u001B[39m     pickle_load_args[\u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1484\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[32m   1485\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[32m   1486\u001B[39m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[32m   1487\u001B[39m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[32m   1488\u001B[39m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[32m   1489\u001B[39m         orig_position = opened_file.tell()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\serialization.py:759\u001B[39m, in \u001B[36m_open_file_like\u001B[39m\u001B[34m(name_or_buffer, mode)\u001B[39m\n\u001B[32m    757\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_open_file_like\u001B[39m(name_or_buffer: FileLike, mode: \u001B[38;5;28mstr\u001B[39m) -> _opener[IO[\u001B[38;5;28mbytes\u001B[39m]]:\n\u001B[32m    758\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[32m--> \u001B[39m\u001B[32m759\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    760\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    761\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\serialization.py:740\u001B[39m, in \u001B[36m_open_file.__init__\u001B[39m\u001B[34m(self, name, mode)\u001B[39m\n\u001B[32m    739\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: Union[\u001B[38;5;28mstr\u001B[39m, os.PathLike[\u001B[38;5;28mstr\u001B[39m]], mode: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m740\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: './models/model_epoch_XX.pth'"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f9449a8219d9f96c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8d2f3e66063097e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 }
}
